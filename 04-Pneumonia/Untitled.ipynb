{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77277a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24000 train images and 2684 val images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | model     | ResNet            | 11.2 M\n",
      "1 | loss_fn   | BCEWithLogitsLoss | 0     \n",
      "2 | train_acc | BinaryAccuracy    | 0     \n",
      "3 | val_acc   | BinaryAccuracy    | 0     \n",
      "------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.683    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12a425f2608412bbeb70c2b911bc92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def load_file(path):\n",
    "    return np.load(path).astype(np.float32)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                                    transforms.ToTensor(),  # Convert numpy array to tensor\n",
    "                                    transforms.Normalize(0.49, 0.248),  # Use mean and std from preprocessing notebook\n",
    "                                    transforms.RandomAffine( # Data Augmentation\n",
    "                                        degrees=(-5, 5), translate=(0, 0.05), scale=(0.9, 1.1)),\n",
    "                                        transforms.RandomResizedCrop((224, 224), scale=(0.35, 1))\n",
    "\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "                                    transforms.ToTensor(),  # Convert numpy array to tensor\n",
    "                                    transforms.Normalize([0.49], [0.248]),  # Use mean and std from preprocessing notebook\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.DatasetFolder(\n",
    "    \"C:/Users/Owner/Downloads/Udemy/Pytorch/04-Pneumonia-Classification/04-Pneumonia-Classification/Processed/train/\",\n",
    "    loader=load_file, extensions=\"npy\", transform=train_transforms)\n",
    "\n",
    "val_dataset = torchvision.datasets.DatasetFolder(\n",
    "    \"C:/Users/Owner/Downloads/Udemy/Pytorch/04-Pneumonia-Classification/04-Pneumonia-Classification/Processed/val/\",\n",
    "    loader=load_file, extensions=\"npy\", transform=val_transforms)\n",
    "\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "train_dataset\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "val_dataset\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "fig, axis = plt.subplots(2, 2, figsize=(9, 9))\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        random_index = np.random.randint(0, 20000)\n",
    "        x_ray, label = train_dataset[random_index]\n",
    "        axis[i][j].imshow(x_ray[0], cmap=\"bone\")\n",
    "        axis[i][j].set_title(f\"Label:{label}\")\n",
    "\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size= batch_size, num_workers=0, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "print(f\"There are {len(train_dataset)} train images and {len(val_dataset)} val images\")\n",
    "\n",
    "\n",
    "# The classes are imbalanced: There are more images without signs of pneumonia than with pneumonia.\n",
    "# There are multiple ways to deal with imbalanced datasets:\n",
    "\n",
    "    #Weighted Loss\n",
    "    #Oversampling\n",
    "    #Doing nothing :)\n",
    "\n",
    "#In this example, we will simply do nothing as this often yields the best results. \n",
    "# Buf feel free to play around with a weighted loss. A template to define a customized weighted loss function is provided below.\n",
    "\n",
    "#Oversampling will be shown in a later lecture.\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "np.unique(train_dataset.targets, return_counts=True), np.unique(val_dataset.targets, return_counts=True)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "torchvision.models.resnet18()\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# Important: Lighting pytorch training model \n",
    "\n",
    "class PneumoniaModel(pl.LightningModule):\n",
    "    def __init__(self, weight=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = torchvision.models.resnet18()\n",
    "        # change conv1 from 3 to 1 input channels\n",
    "        self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        # change out_feature of the last fully connected layer (called fc in resnet18) from 1000 to 1\n",
    "        self.model.fc = torch.nn.Linear(in_features=512, out_features=1)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([weight]))\n",
    "\n",
    "        # simple accuracy computation\n",
    "        self.train_acc = torchmetrics.Accuracy(task='binary')\n",
    "        self.val_acc = torchmetrics.Accuracy(task='binary')\n",
    "\n",
    "    def forward(self, data):\n",
    "        pred = self.model(data)\n",
    "        return pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_ray, label = batch\n",
    "        label = label.float()  # Convert label to float (just needed for loss computation)\n",
    "        pred = self(x_ray)[:, 0]  # Prediction: Make sure prediction and label have same shape\n",
    "        loss = self.loss_fn(pred, label)  # Compute the loss\n",
    "\n",
    "        # Log loss and batch accuracy\n",
    "        self.log(\"Train Loss\", loss)\n",
    "        self.log(\"Step Train Acc\", self.train_acc(torch.sigmoid(pred), label.int()))\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self, outs):\n",
    "        # After one epoch compute the whole train_data accuracy\n",
    "        self.log(\"Train Acc\", self.train_acc.compute())\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Same steps as in the training_step\n",
    "        x_ray, label = batch\n",
    "        label = label.float()\n",
    "        pred = self(x_ray)[:, 0]  # make sure prediction and label have same shape\n",
    "\n",
    "        loss = self.loss_fn(pred, label)\n",
    "\n",
    "        # Log validation metrics\n",
    "        self.log(\"Val Loss\", loss)\n",
    "        self.log(\"Step Val Acc\", self.val_acc(torch.sigmoid(pred), label.int()))\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end_epoch_end(self, trainer, pl_module):\n",
    "        self.log(\"Val Acc\", self.val_acc.compute())\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Caution! You always need to return a list here (just pack your optimizer into one :))\n",
    "        return [self.optimizer]\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "model = PneumoniaModel()  # Instanciate the model\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# Create the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val Acc',\n",
    "    save_top_k=10,\n",
    "    mode='max')\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(accelerator='gpu', max_epochs=35)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu', \n",
    "    logger=TensorBoardLogger(save_dir=\"C:/Users/Owner/Downloads/Udemy/Pytorch/04-Pneumonia-Classification/04-Pneumonia-Classification/logs\"), \n",
    "    log_every_n_steps=1,\n",
    "    callbacks=checkpoint_callback,\n",
    "    max_epochs=35\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def custom_validation_epoch_end(trainer, model, outs):\n",
    "    model.log(\"Val Acc\", model.val_acc.compute())\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use strict=False, otherwise we would want to match the pos_weight which is not necessary\n",
    "model = PneumoniaModel.load_from_checkpoint(\"weights/weights_1.ckpt\")\n",
    "model.eval()\n",
    "model.to(device);\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in tqdm(val_dataset):\n",
    "        data = data.to(device).float().unsqueeze(0)\n",
    "        pred = torch.sigmoid(model(data)[0].cpu())\n",
    "        preds.append(pred)\n",
    "        labels.append(label)\n",
    "preds = torch.tensor(preds)\n",
    "labels = torch.tensor(labels).int()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97317bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
